{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('data_set_anonymized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 2:11].values\n",
    "y = dataset.iloc[:, 11].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_1 = sc.fit_transform(X_train)\n",
    "X_test_1 = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Modules. We need Sequential module for initializing NN and dense module to add Hidden Layers.\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving the name of model as Classifier as our business problem is the classification of customer churn\n",
    "#Initializing Neural Network\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=9, units=5, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=5, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Our first parameter is output_dim. It is simply the number of nodes you want to add to this layer. \n",
    "init is the initialization of Stochastic Gradient Decent.\n",
    "In Neural Network we need to assign weights to each mode which is nothing but importance of that node.\n",
    "At the time of initialization, weights should be close to 0 and we will randomly initialize weights using uniform function. \n",
    "input_dim parameter is needed only for first layer as model doesn’t know the number of our input variables. \n",
    "Remember in our case, the total number of input variables are 9. \n",
    "In the second layer model automatically knows the number of input variable from the first hidden layer.\n",
    "\"\"\"\"\n",
    "\"\"\"\"\n",
    "Talking About the Activation Function,Neuron applies activation function to weighted sum(summation of Wi * Xi where w is weight, X is input variable and i is suffix of W and X). \n",
    "The closer the activation function value to 1 the more activated is the neuron and more the neuron passes the signal.  \n",
    "Here we are using rectifier(relu) function in our hidden layer and Sigmoid function in our output layer as we want binary result\n",
    "\"\"\"\"\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 5, init = 'uniform', activation = 'relu', input_dim = 9))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 5, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "First argument is Optimizer, this is nothing but the algorithm you wanna use to find optimal set of weights(Note that in step 9 we just\n",
    "initialized weights now we are applying some sort of algorithm which will optimize weights\n",
    "in turn making out neural network more powerful. \n",
    "This algorithm is Stochastic Gradient descent(SGD). \n",
    "Among several types of SGD algorithm the one which we will use is ‘Adam’.\n",
    "If you go in deeper detail of SGD, you will find that SGD depends on loss thus our second parameter is loss. Since out dependent variable is binary, we will have to use logarithmic loss function called ‘binary_crossentropy’\n",
    "\"\"\"\"\n",
    "\n",
    "# Compiling Neural Network\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42268/42268 [==============================] - 4s 101us/step - loss: 0.1420 - acc: 0.9380\n",
      "Epoch 2/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0561 - acc: 0.9764\n",
      "Epoch 3/100\n",
      "42268/42268 [==============================] - 4s 91us/step - loss: 0.0449 - acc: 0.9852\n",
      "Epoch 4/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0346 - acc: 0.9924\n",
      "Epoch 5/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0264 - acc: 0.9946\n",
      "Epoch 6/100\n",
      "42268/42268 [==============================] - 4s 94us/step - loss: 0.0208 - acc: 0.9956\n",
      "Epoch 7/100\n",
      "42268/42268 [==============================] - 4s 99us/step - loss: 0.0163 - acc: 0.9962\n",
      "Epoch 8/100\n",
      "42268/42268 [==============================] - 4s 99us/step - loss: 0.0134 - acc: 0.9971\n",
      "Epoch 9/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0116 - acc: 0.9970\n",
      "Epoch 10/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0108 - acc: 0.9971\n",
      "Epoch 11/100\n",
      "42268/42268 [==============================] - 4s 94us/step - loss: 0.0104 - acc: 0.9969\n",
      "Epoch 12/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0106 - acc: 0.9970\n",
      "Epoch 13/100\n",
      "42268/42268 [==============================] - 4s 95us/step - loss: 0.0107 - acc: 0.9968\n",
      "Epoch 14/100\n",
      "42268/42268 [==============================] - 4s 98us/step - loss: 0.0083 - acc: 0.9976\n",
      "Epoch 15/100\n",
      "42268/42268 [==============================] - 4s 98us/step - loss: 0.0108 - acc: 0.9969\n",
      "Epoch 16/100\n",
      "42268/42268 [==============================] - 4s 98us/step - loss: 0.0098 - acc: 0.9974\n",
      "Epoch 17/100\n",
      "42268/42268 [==============================] - 4s 99us/step - loss: 0.0090 - acc: 0.9973\n",
      "Epoch 18/100\n",
      "42268/42268 [==============================] - 4s 98us/step - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 19/100\n",
      "42268/42268 [==============================] - 4s 99us/step - loss: 0.0096 - acc: 0.9974\n",
      "Epoch 20/100\n",
      "42268/42268 [==============================] - 4s 97us/step - loss: 0.0104 - acc: 0.9971\n",
      "Epoch 21/100\n",
      "42268/42268 [==============================] - 4s 97us/step - loss: 0.0076 - acc: 0.9975\n",
      "Epoch 22/100\n",
      "42268/42268 [==============================] - 4s 95us/step - loss: 0.0090 - acc: 0.9976\n",
      "Epoch 23/100\n",
      "42268/42268 [==============================] - 4s 95us/step - loss: 0.0082 - acc: 0.9974\n",
      "Epoch 24/100\n",
      "42268/42268 [==============================] - 4s 91us/step - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 25/100\n",
      "42268/42268 [==============================] - 4s 90us/step - loss: 0.0082 - acc: 0.9979\n",
      "Epoch 26/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0094 - acc: 0.9972\n",
      "Epoch 27/100\n",
      "42268/42268 [==============================] - 4s 90us/step - loss: 0.0090 - acc: 0.9977\n",
      "Epoch 28/100\n",
      "42268/42268 [==============================] - 4s 86us/step - loss: 0.0083 - acc: 0.9978\n",
      "Epoch 29/100\n",
      "42268/42268 [==============================] - 4s 90us/step - loss: 0.0080 - acc: 0.9977\n",
      "Epoch 30/100\n",
      "42268/42268 [==============================] - 4s 91us/step - loss: 0.0081 - acc: 0.9978\n",
      "Epoch 31/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 32/100\n",
      "42268/42268 [==============================] - 4s 97us/step - loss: 0.0099 - acc: 0.9975\n",
      "Epoch 33/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0075 - acc: 0.9974\n",
      "Epoch 34/100\n",
      "42268/42268 [==============================] - 4s 97us/step - loss: 0.0101 - acc: 0.9978\n",
      "Epoch 35/100\n",
      "42268/42268 [==============================] - 4s 98us/step - loss: 0.0077 - acc: 0.9980\n",
      "Epoch 36/100\n",
      "42268/42268 [==============================] - 4s 100us/step - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 37/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0095 - acc: 0.9975\n",
      "Epoch 38/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 39/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0089 - acc: 0.9976\n",
      "Epoch 40/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 41/100\n",
      "42268/42268 [==============================] - 4s 89us/step - loss: 0.0072 - acc: 0.9983\n",
      "Epoch 42/100\n",
      "42268/42268 [==============================] - 4s 100us/step - loss: 0.0067 - acc: 0.9980\n",
      "Epoch 43/100\n",
      "42268/42268 [==============================] - 4s 85us/step - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 44/100\n",
      "42268/42268 [==============================] - 4s 86us/step - loss: 0.0068 - acc: 0.9981\n",
      "Epoch 45/100\n",
      "42268/42268 [==============================] - 4s 91us/step - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 46/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0087 - acc: 0.9981\n",
      "Epoch 47/100\n",
      "42268/42268 [==============================] - 4s 91us/step - loss: 0.0070 - acc: 0.9980\n",
      "Epoch 48/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0074 - acc: 0.9979\n",
      "Epoch 49/100\n",
      "42268/42268 [==============================] - 4s 97us/step - loss: 0.0052 - acc: 0.9982\n",
      "Epoch 50/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0071 - acc: 0.9981\n",
      "Epoch 51/100\n",
      "42268/42268 [==============================] - 4s 97us/step - loss: 0.0076 - acc: 0.9981\n",
      "Epoch 52/100\n",
      "42268/42268 [==============================] - 4s 97us/step - loss: 0.0065 - acc: 0.9985\n",
      "Epoch 53/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0078 - acc: 0.9979\n",
      "Epoch 54/100\n",
      "42268/42268 [==============================] - 4s 95us/step - loss: 0.0070 - acc: 0.9981\n",
      "Epoch 55/100\n",
      "42268/42268 [==============================] - 4s 87us/step - loss: 0.0083 - acc: 0.9977\n",
      "Epoch 56/100\n",
      "42268/42268 [==============================] - 4s 89us/step - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 57/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0050 - acc: 0.9988\n",
      "Epoch 58/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0079 - acc: 0.9981\n",
      "Epoch 59/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 60/100\n",
      "42268/42268 [==============================] - 4s 94us/step - loss: 0.0065 - acc: 0.9980\n",
      "Epoch 61/100\n",
      "42268/42268 [==============================] - 4s 91us/step - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 62/100\n",
      "42268/42268 [==============================] - 4s 86us/step - loss: 0.0074 - acc: 0.9982\n",
      "Epoch 63/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0065 - acc: 0.9981\n",
      "Epoch 64/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0065 - acc: 0.9983\n",
      "Epoch 65/100\n",
      "42268/42268 [==============================] - 4s 94us/step - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 66/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 67/100\n",
      "42268/42268 [==============================] - 4s 98us/step - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 68/100\n",
      "42268/42268 [==============================] - 4s 95us/step - loss: 0.0075 - acc: 0.9978\n",
      "Epoch 69/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0065 - acc: 0.9983\n",
      "Epoch 70/100\n",
      "42268/42268 [==============================] - 4s 94us/step - loss: 0.0045 - acc: 0.9986\n",
      "Epoch 71/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0056 - acc: 0.9985\n",
      "Epoch 72/100\n",
      "42268/42268 [==============================] - 4s 86us/step - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 73/100\n",
      "42268/42268 [==============================] - 4s 85us/step - loss: 0.0062 - acc: 0.9983\n",
      "Epoch 74/100\n",
      "42268/42268 [==============================] - 4s 86us/step - loss: 0.0058 - acc: 0.9983\n",
      "Epoch 75/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0078 - acc: 0.9981\n",
      "Epoch 76/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0056 - acc: 0.9981\n",
      "Epoch 77/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0062 - acc: 0.9983\n",
      "Epoch 78/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0060 - acc: 0.9984\n",
      "Epoch 79/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0064 - acc: 0.9984\n",
      "Epoch 80/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0077 - acc: 0.9981\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0052 - acc: 0.9986\n",
      "Epoch 82/100\n",
      "42268/42268 [==============================] - 4s 95us/step - loss: 0.0063 - acc: 0.9980\n",
      "Epoch 83/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0067 - acc: 0.9983\n",
      "Epoch 84/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 85/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 86/100\n",
      "42268/42268 [==============================] - 4s 99us/step - loss: 0.0070 - acc: 0.9979\n",
      "Epoch 87/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 88/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0071 - acc: 0.9982\n",
      "Epoch 89/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 90/100\n",
      "42268/42268 [==============================] - 4s 90us/step - loss: 0.0047 - acc: 0.9987\n",
      "Epoch 91/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 92/100\n",
      "42268/42268 [==============================] - 4s 89us/step - loss: 0.0047 - acc: 0.9986\n",
      "Epoch 93/100\n",
      "42268/42268 [==============================] - 4s 85us/step - loss: 0.0059 - acc: 0.9985\n",
      "Epoch 94/100\n",
      "42268/42268 [==============================] - 4s 89us/step - loss: 0.0068 - acc: 0.9983\n",
      "Epoch 95/100\n",
      "42268/42268 [==============================] - 4s 96us/step - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 96/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 97/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0057 - acc: 0.9982\n",
      "Epoch 98/100\n",
      "42268/42268 [==============================] - 4s 92us/step - loss: 0.0056 - acc: 0.9986\n",
      "Epoch 99/100\n",
      "42268/42268 [==============================] - 4s 93us/step - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 100/100\n",
      "42268/42268 [==============================] - 4s 91us/step - loss: 0.0043 - acc: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff355ad978>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Batch size is used to specify the number of observation after which you want to update weight. \n",
    "Epoch is nothing but the total number of iterations.\n",
    "Choosing the value of batch size and epoch is trial and error there is no specific rule for that.\n",
    "\"\"\"\"    \n",
    "#Fitting our model \n",
    "classifier.fit(X_train_1, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999 ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " ...\n",
      " [0.99999976]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test_1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(y_pred)-1):\n",
    "    if y_pred[i] == 1 :\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred [i] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6684    0]\n",
      " [2447 1437]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tenure  sessions    duration   mega_bytes  case_count  comp_flag  \\\n",
      "0        59.0     794.0   629117.18    483240.36         0.0        0.0   \n",
      "1       243.0    2864.0  2618037.41   3635191.34         0.0        0.0   \n",
      "2       120.0     553.0   512110.97    233690.76         0.0        0.0   \n",
      "3       137.0     847.0   708583.82    239119.29         0.0        0.0   \n",
      "4       120.0    3774.0  2697814.43   4538082.33         0.0        0.0   \n",
      "5        61.0      46.0    46216.70     19675.85         0.0        0.0   \n",
      "6        92.0     363.0   289591.73    456426.78         0.0        0.0   \n",
      "7       150.0     665.0   394792.27    996021.31         0.0        0.0   \n",
      "8       295.0    1781.0   925440.00   1250080.17         0.0        1.0   \n",
      "9       133.0    1087.0   654358.48    344707.40         0.0        0.0   \n",
      "10      181.0     643.0   521102.83   1020040.27         0.0        0.0   \n",
      "11      158.0     676.0   718044.01   1391991.12         0.0        0.0   \n",
      "12       62.0    4754.0  3006985.46   4019035.70         0.0        1.0   \n",
      "13       30.0     272.0   246988.83    183000.58         0.0        0.0   \n",
      "14      303.0    1411.0  1359789.15    880499.78         4.0        0.0   \n",
      "15       89.0      35.0    12458.78     43246.66         0.0        0.0   \n",
      "16       92.0    1566.0  1400048.03    485740.76         0.0        0.0   \n",
      "17       61.0     110.0   105688.70     66021.30         0.0        0.0   \n",
      "18       31.0     114.0    83293.98    123288.96         1.0        0.0   \n",
      "19      153.0     476.0   425671.38    844299.34         0.0        0.0   \n",
      "20       61.0     597.0   598867.68    606625.37         0.0        0.0   \n",
      "21       62.0     411.0   378948.28    332830.23         0.0        0.0   \n",
      "22      363.0       3.0     1805.73      3921.05         2.0        0.0   \n",
      "23      122.0     677.0   530685.25    255194.38         0.0        0.0   \n",
      "24       62.0      71.0    25342.35     22291.47         0.0        0.0   \n",
      "25       59.0     668.0   596406.44    584156.15         0.0        0.0   \n",
      "26      122.0     286.0   238564.57   1114220.57         0.0        0.0   \n",
      "27      434.0    1845.0  1088081.48   2318680.34         0.0        0.0   \n",
      "28      219.0    4170.0  3756084.57   1550261.68         0.0        0.0   \n",
      "29      122.0     288.0   285485.92    401531.54         0.0        0.0   \n",
      "...       ...       ...         ...          ...         ...        ...   \n",
      "10538   122.0     282.0   292054.18    486354.48         0.0        0.0   \n",
      "10539   331.0    3964.0  3647699.32   3295307.13         0.0        0.0   \n",
      "10540    92.0     467.0   442889.18   1430353.38         4.0        0.0   \n",
      "10541   136.0     134.0    93951.33    317423.71         0.0        1.0   \n",
      "10542    31.0     785.0   643527.13   1911032.37         0.0        0.0   \n",
      "10543   120.0     754.0   420863.93    559917.90         4.0        0.0   \n",
      "10544    61.0     609.0   563006.62    672035.74         0.0        0.0   \n",
      "10545    61.0     156.0    98181.35     40695.72         0.0        0.0   \n",
      "10546    61.0     152.0   180251.38    267858.57         0.0        0.0   \n",
      "10547    30.0     114.0    88123.22     65791.67         0.0        0.0   \n",
      "10548   151.0    1155.0  1242371.61   1705528.26         0.0        0.0   \n",
      "10549   112.0     840.0   734246.98    667261.04         0.0        0.0   \n",
      "10550    62.0     185.0   171176.05    150360.77         0.0        0.0   \n",
      "10551   122.0    1786.0  1361651.94    785241.04         0.0        0.0   \n",
      "10552   181.0   12530.0  6801701.42  10668412.14         0.0        0.0   \n",
      "10553   102.0     752.0   759779.65   1118388.44         0.0        0.0   \n",
      "10554   106.0     268.0   208035.98    199254.43         0.0        0.0   \n",
      "10555    59.0     494.0   366461.30    421019.40         0.0        0.0   \n",
      "10556   153.0     736.0   595105.96    494522.81         0.0        0.0   \n",
      "10557    62.0     112.0    80040.35    255818.13         0.0        1.0   \n",
      "10558    92.0     476.0   438002.30    338419.72         0.0        0.0   \n",
      "10559   181.0    1629.0  1138776.37    719629.37         0.0        0.0   \n",
      "10560   214.0    1869.0  1777815.61   2438939.40         0.0        0.0   \n",
      "10561   123.0     569.0   591444.50   1380412.69         4.0        0.0   \n",
      "10562   154.0    1087.0  1006599.61    492935.11         0.0        0.0   \n",
      "10563   181.0    1123.0   937012.01   1304146.24         0.0        0.0   \n",
      "10564   120.0    1416.0  1417599.55    988091.10         0.0        0.0   \n",
      "10565    61.0     737.0   784307.16    738047.17         0.0        0.0   \n",
      "10566   274.0     552.0   392498.58    910503.13         0.0        0.0   \n",
      "10567   136.0     627.0   597737.88    984015.88         0.0        1.0   \n",
      "\n",
      "       train_flag  pcs_flag  holiday_flag  çhurn  churn_predicted  \n",
      "0             0.0       0.0           0.0      1              0.0  \n",
      "1             1.0       1.0           0.0      0              0.0  \n",
      "2             1.0       1.0           0.0      0              0.0  \n",
      "3             1.0       0.0           0.0      0              0.0  \n",
      "4             1.0       0.0           0.0      0              0.0  \n",
      "5             1.0       1.0           0.0      1              0.0  \n",
      "6             1.0       0.0           0.0      0              0.0  \n",
      "7             1.0       0.0           1.0      0              0.0  \n",
      "8             0.0       0.0           0.0      0              0.0  \n",
      "9             1.0       0.0           0.0      0              0.0  \n",
      "10            0.0       1.0           0.0      0              0.0  \n",
      "11            1.0       0.0           0.0      0              0.0  \n",
      "12            0.0       0.0           0.0      1              1.0  \n",
      "13            0.0       1.0           0.0      1              1.0  \n",
      "14            0.0       0.0           0.0      0              0.0  \n",
      "15            1.0       1.0           0.0      1              0.0  \n",
      "16            1.0       0.0           0.0      0              0.0  \n",
      "17            1.0       1.0           0.0      1              0.0  \n",
      "18            1.0       0.0           1.0      1              1.0  \n",
      "19            1.0       0.0           0.0      0              0.0  \n",
      "20            1.0       0.0           0.0      1              0.0  \n",
      "21            0.0       0.0           0.0      1              0.0  \n",
      "22            0.0       1.0           0.0      0              0.0  \n",
      "23            1.0       1.0           0.0      0              0.0  \n",
      "24            0.0       0.0           0.0      1              0.0  \n",
      "25            0.0       0.0           0.0      1              0.0  \n",
      "26            0.0       0.0           0.0      0              0.0  \n",
      "27            0.0       1.0           0.0      0              0.0  \n",
      "28            1.0       1.0           0.0      0              0.0  \n",
      "29            1.0       1.0           0.0      0              0.0  \n",
      "...           ...       ...           ...    ...              ...  \n",
      "10538         1.0       0.0           1.0      0              0.0  \n",
      "10539         1.0       1.0           0.0      0              0.0  \n",
      "10540         0.0       0.0           0.0      0              0.0  \n",
      "10541         0.0       0.0           0.0      0              0.0  \n",
      "10542         1.0       0.0           0.0      1              1.0  \n",
      "10543         1.0       1.0           0.0      0              0.0  \n",
      "10544         1.0       1.0           0.0      1              0.0  \n",
      "10545         1.0       1.0           0.0      1              0.0  \n",
      "10546         1.0       0.0           1.0      1              0.0  \n",
      "10547         1.0       1.0           0.0      1              1.0  \n",
      "10548         1.0       0.0           0.0      0              0.0  \n",
      "10549         0.0       0.0           0.0      0              0.0  \n",
      "10550         1.0       0.0           1.0      1              0.0  \n",
      "10551         1.0       0.0           1.0      0              0.0  \n",
      "10552         0.0       1.0           0.0      0              0.0  \n",
      "10553         1.0       1.0           0.0      0              0.0  \n",
      "10554         1.0       1.0           0.0      0              0.0  \n",
      "10555         1.0       0.0           0.0      1              0.0  \n",
      "10556         1.0       0.0           0.0      0              0.0  \n",
      "10557         0.0       0.0           0.0      1              0.0  \n",
      "10558         1.0       0.0           1.0      0              0.0  \n",
      "10559         0.0       0.0           0.0      0              0.0  \n",
      "10560         1.0       0.0           0.0      0              0.0  \n",
      "10561         0.0       0.0           1.0      0              0.0  \n",
      "10562         1.0       0.0           1.0      0              0.0  \n",
      "10563         0.0       1.0           0.0      0              0.0  \n",
      "10564         1.0       0.0           0.0      0              0.0  \n",
      "10565         1.0       0.0           0.0      1              0.0  \n",
      "10566         0.0       0.0           0.0      0              0.0  \n",
      "10567         0.0       1.0           0.0      0              0.0  \n",
      "\n",
      "[10568 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "final_df = pd.DataFrame(list(X_test),columns=['tenure','sessions','duration','mega_bytes','case_count','comp_flag','train_flag','pcs_flag','holiday_flag'\n",
    "])\n",
    "df_1 = pd.DataFrame(list(y_test),columns=['çhurn'])\n",
    "df_2 = pd.DataFrame(list(y_pred),columns=['churn_predicted'])\n",
    "final_dataframe = final_df.join(df_1).join(df_2)\n",
    "print (final_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.to_csv('C:\\\\Users\\\\billu\\\\Churn_Model_final.csv',index=False,encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
